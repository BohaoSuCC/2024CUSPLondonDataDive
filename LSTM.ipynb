{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import package to count number of hours after specific time\n",
    "from datetime import datetime\n",
    "\n",
    "# Faerier, adjusts for modulation, 7, 31\n",
    "# Plot residuals to Faerier plot\n",
    "\n",
    "# Define file paths\n",
    "combined_dataset = \"/home/henry-cao/Desktop/KCL/Extracurriculars/CUSP/Project_Code/Datasets/no_na_dataset.csv\"\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(combined_dataset)\n",
    "\n",
    "# Print col names\n",
    "print(data.columns)\n",
    "\n",
    "# Print number of rows\n",
    "print(\"Number of rows before preprocessing: \", data.shape[0])\n",
    "\n",
    "# Only keep unique rows\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "# Drop na values from Measurement, SatMean, and FlowMean\n",
    "data = data.dropna(subset=['Measurement', 'SatMean', 'FlowMean'])\n",
    "\n",
    "# When SatMean is less than 0, set value to na\n",
    "data.loc[data['SatMean'] < 0, 'SatMean'] = np.nan\n",
    "\n",
    "# Covert time column to datetime\n",
    "data['time_temp'] = pd.to_datetime(data['MeasurementDateGMT'])\n",
    "\n",
    "# Find earliest timestamp\n",
    "earliest_timestamp = data['time_temp'].min()\n",
    "\n",
    "# Calculated time elapsed\n",
    "data['time_elapsed'] = data['time_temp'] - earliest_timestamp\n",
    "\n",
    "# Turn time_elapsed into float32\n",
    "data['time_elapsed'] = data['time_elapsed'].dt.total_seconds().astype('float32')\n",
    "\n",
    "# Drop MeasurementDateGMT and time columns\n",
    "data = data.drop(columns=['time_temp'])\n",
    "\n",
    "# Convert certain categorical variables to numerical encoded values\n",
    "data['SiteType'] = data['SiteType'].astype('category').cat.codes\n",
    "\n",
    "# Store unique values of SpeciesType\n",
    "unique_species_values = data['SpeciesType'].unique()\n",
    "\n",
    "# Convert SpeciesType to numerical encoded values, and link up unique_species_values w/ numerical values\n",
    "data['SpeciesType'] = data['SpeciesType'].astype('category').cat.codes\n",
    "unique_species_dict = {i: unique_species_values[i] for i in range(len(unique_species_values))}\n",
    "\n",
    "# Turn time, which represents exact hour of the day, into numerical\n",
    "data['Hour'] = data['MeasurementDateGMT'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M').hour).astype(int)\n",
    "\n",
    "# Extract day, month, and year from MeasurmentDateGMT\n",
    "# Day is in index 0-1, month is in index 5-7, year is in index 9-12\n",
    "# Time is in format YYYY-MM-DD HH:MM\n",
    "data['Day'] = data['MeasurementDateGMT'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M').day).astype(int)\n",
    "data['Month'] = data['MeasurementDateGMT'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M').month).astype(int)\n",
    "data['Year'] = data['MeasurementDateGMT'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M').year).astype(int)\n",
    "\n",
    "# Dropping irrelevant or redundant columns\n",
    "data = data.drop(columns=['MeasurementDateGMT', 'LocalAuthorityName', 'SiteCode', 'LocalAuthorityCode', 'SiteName', 'DateOpened', 'DateClosed', 'ID', 'DateTime', 'date',  'Date', 'SatBand', 'DateTimeStr', 'Time'])\n",
    "\n",
    "# Print number of rows\n",
    "print(\"Number of rows after preprocessing: \", data.shape[0])\n",
    "\n",
    "# Define input size annd output size based on data shape\n",
    "num_features = data.shape[1] - 1\n",
    "\n",
    "# Making sure dropped columns are gone\n",
    "print(f\"Columns remaining: {data.columns}\")\n",
    "\n",
    "# Make test and training sets, with Measurements as response variable and\n",
    "# all other variables as predictors\n",
    "X = data.drop(columns=['Measurement'])\n",
    "y = data['Measurement']\n",
    "\n",
    "# Converting datatypes to float32\n",
    "X = X.astype('float32')\n",
    "y = y.astype('float32')\n",
    "\n",
    "# # Initialise MinMaxScaler\n",
    "# scaler = MinMaxScaler(feature_range=(0,1))\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Separate by SpeciesType\n",
    "unique_species = data['SpeciesType'].unique()\n",
    "for species in unique_species:\n",
    "    # Use unique_species_dict to print species\n",
    "    species_string = unique_species_dict[species]\n",
    "    print(f\"Pollutant: {species_string}\")\n",
    "\n",
    "    # Filter data by species\n",
    "    data_species = data[data['SpeciesType'] == species]\n",
    "\n",
    "    # Make test and training sets, with Measurements as response variable\n",
    "    X_species = data_species.drop(columns=['Measurement', 'SpeciesType'])\n",
    "    y_species = data_species['Measurement']\n",
    "    X_species = X_species.astype('float32')\n",
    "    y_species = y_species.astype('float32')\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_species)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_species, test_size=0.2, random_state=1)\n",
    "\n",
    "    X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "    model = Sequential([\n",
    "        LSTM(units=500, activation='relu', input_shape=(1, X_train.shape[2]), recurrent_initializer='glorot_uniform'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001, clipvalue=0.5), loss='mean_squared_error')\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    checkpoint_filepath = f\"/home/henry-cao/Desktop/KCL/Extracurriculars/CUSP/Project_Code/CUSP_source/LSTM_Models/checkpoint_model_{species_string}.h5\"\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=False,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_best_only=True\n",
    "    )\n",
    "\n",
    "    # Fit model\n",
    "    history = model.fit(X_train, y_train, epochs=100, batch_size=256, validation_split = 0.2, callbacks=[model_checkpoint_callback])\n",
    "\n",
    "    # Load best model\n",
    "    best_model = load_model(checkpoint_filepath)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Plot Residuals between real and predicted values\n",
    "    y_residuals = y_test - y_pred.flatten()\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(y_test, y_residuals, color='blue', label='Residuals')\n",
    "    plt.axhline(y=0, color='red', linestyle='--')\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title(f'Residuals for {species_string}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(history.history['loss'], label='Training loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "    plt.title('Loss for {species_string}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # # Calculate and print accuracy of predictions\n",
    "    MSE = sk.metrics.mean_squared_error(y_test, y_pred)\n",
    "    print(\"Mean squared error for {species}: \", MSE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Initialise StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# # Fit X to have mean 0 and variance 1\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # Split data into training and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "# X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# model = Sequential([\n",
    "#     LSTM(units=500, activation='relu', input_shape=(1, X_train.shape[2]), recurrent_initializer='glorot_uniform'),\n",
    "#     Dense(1)\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer=Adam(learning_rate=0.001, clipvalue=0.5), loss='mean_squared_error')\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# checkpoint_filepath = '/home/henry-cao/Desktop/KCL/Extracurriculars/CUSP/Project_Code/CUSP_source/LSTM_Models/checkpoint_model.h5'\n",
    "# model_checkpoint_callback = ModelCheckpoint(\n",
    "#     filepath=checkpoint_filepath,\n",
    "#     save_weights_only=False,\n",
    "#     monitor='val_loss',\n",
    "#     mode='min',\n",
    "#     save_best_only=True\n",
    "# )\n",
    "\n",
    "# # Fit model\n",
    "# history = model.fit(X_train, y_train, epochs=100, batch_size=256, validation_split = 0.2, callbacks=[model_checkpoint_callback])\n",
    "\n",
    "# # Load best model\n",
    "# best_model = load_model(checkpoint_filepath)\n",
    "\n",
    "# # Make predictions\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # Plot Residuals between real and predicted values\n",
    "# y_residuals = y_test - y_pred.flatten()\n",
    "# plt.figure(figsize=(10,6))\n",
    "# plt.scatter(y_test, y_residuals, color='blue', label='Residuals')\n",
    "# plt.axhline(y=0, color='red', linestyle='--')\n",
    "# plt.xlabel('Actual Values')\n",
    "# plt.ylabel('Residuals')\n",
    "# plt.title('Residuals')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Plot accuracy\n",
    "# plt.figure(figsize=(10,6))\n",
    "# plt.plot(history.history['loss'], label='Training loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "# plt.title('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Calculate and print accuracy of predictions\n",
    "MSE = sk.metrics.mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean squared error: \", MSE)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
