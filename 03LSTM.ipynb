{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import package to count number of hours after specific time\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cleaning and Converting the datatype**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Faerier, adjusts for modulation, 7, 31\n",
    "# Plot residuals to Faerier plot\n",
    "\n",
    "# Define file paths\n",
    "combined_dataset = \"Data/DataForModel.csv\"\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(combined_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Measurement', 'SiteCode', 'LocalAuthorityCode', 'SiteType',\n",
       "       'SpeciesType', 'SatMean', 'SatBand', 'FlowMean', 'temperature_2m (°C)',\n",
       "       'relative_humidity_2m (%)', 'wind_direction_10m (°)', 'HourOfDay',\n",
       "       'DayOfMonth', 'DayOfWeek', 'Distance', 'Bearing', 'Hours', 'Days'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对data中所有数值类型变量进行标准化处理\n",
    "\n",
    "# Select columns to standardize\n",
    "cols_to_standardize = ['Measurement', 'SatMean', 'FlowMean', 'temperature_2m (°C)', 'relative_humidity_2m (%)', 'wind_direction_10m (°)', 'HourOfDay', 'DayOfMonth', 'DayOfWeek', 'Distance', 'Bearing', 'Hours', 'Days']\n",
    "\n",
    "# Standardize columns\n",
    "scaler = StandardScaler()\n",
    "data[cols_to_standardize] = scaler.fit_transform(data[cols_to_standardize])\n",
    "\n",
    "# select all the categorical variables\n",
    "cols_to_categorize = ['SiteCode', 'LocalAuthorityCode', 'SiteType','SpeciesType','SatBand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols_to_categorize:\n",
    "    data[col] = data[col].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将SatBand列中所有的-1值替换为4\n",
    "data['SatBand'] = data['SatBand'].replace(-1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 389033 entries, 0 to 389032\n",
      "Data columns (total 18 columns):\n",
      " #   Column                    Non-Null Count   Dtype  \n",
      "---  ------                    --------------   -----  \n",
      " 0   Measurement               389033 non-null  float64\n",
      " 1   SiteCode                  389033 non-null  int8   \n",
      " 2   LocalAuthorityCode        389033 non-null  int8   \n",
      " 3   SiteType                  389033 non-null  int8   \n",
      " 4   SpeciesType               389033 non-null  int8   \n",
      " 5   SatMean                   389033 non-null  float64\n",
      " 6   SatBand                   389033 non-null  int8   \n",
      " 7   FlowMean                  389033 non-null  float64\n",
      " 8   temperature_2m (°C)       389033 non-null  float64\n",
      " 9   relative_humidity_2m (%)  389033 non-null  float64\n",
      " 10  wind_direction_10m (°)    389033 non-null  float64\n",
      " 11  HourOfDay                 389033 non-null  float64\n",
      " 12  DayOfMonth                389033 non-null  float64\n",
      " 13  DayOfWeek                 389033 non-null  float64\n",
      " 14  Distance                  389033 non-null  float64\n",
      " 15  Bearing                   389033 non-null  float64\n",
      " 16  Hours                     389033 non-null  float64\n",
      " 17  Days                      389033 non-null  float64\n",
      "dtypes: float64(13), int8(5)\n",
      "memory usage: 40.4 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 96 97 SiteCode\n",
      "0 28 29 LocalAuthorityCode\n",
      "0 4 5 SiteType\n",
      "0 3 4 SpeciesType\n",
      "0 4 5 SatBand\n"
     ]
    }
   ],
   "source": [
    "for col in cols_to_categorize:\n",
    "    print(data[col].min(), data[col].max(), data[col].nunique(), col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **We have successfully convert all variables into float or int, now we are going to re-organize the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the X and y for preparing the data before training the model\n",
    "collist = data.columns.tolist()\n",
    "collist.remove('Measurement')\n",
    "X = data[collist].values\n",
    "y = data['Measurement'].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataset(df, sequence_length):\n",
    "    categorical_sequences = []\n",
    "    continuous_sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    categorical_columns = [1, 2, 3, 4, 6]  \n",
    "    continuous_columns = [i for i in range(1, df.shape[1]) if i not in categorical_columns]  \n",
    "\n",
    "    for i in range(len(df) - sequence_length):\n",
    "        seq = df.iloc[i:i+sequence_length] \n",
    "        \n",
    "        # separate the sequence into categorical and continuous data\n",
    "        categorical_seq = seq.iloc[:, categorical_columns].values\n",
    "        continuous_seq = seq.iloc[:, continuous_columns].values\n",
    "        \n",
    "        # get label\n",
    "        label = df.iloc[i+sequence_length, 0]  \n",
    "        \n",
    "        categorical_sequences.append(categorical_seq)\n",
    "        continuous_sequences.append(continuous_seq)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return np.array(categorical_sequences), np.array(continuous_sequences), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1) # add one dimension to y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X_categorical, X_continuous, y):\n",
    "        self.X_categorical = X_categorical\n",
    "        self.X_continuous = X_continuous\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_categorical[idx], self.X_continuous[idx], self.y[idx]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 24  # 时间窗口长度\n",
    "X_categorical, X_continuous, y = create_dataset(data, sequence_length)\n",
    "\n",
    "dataset = TimeSeriesDataset(X_categorical, X_continuous, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算训练集大小\n",
    "train_size = int(len(y) * 0.8)  # 保留80%的数据用于训练\n",
    "\n",
    "# 分割数据\n",
    "X_cat_train, X_cat_test = X_categorical[:train_size], X_categorical[train_size:]\n",
    "X_cont_train, X_cont_test = X_continuous[:train_size], X_continuous[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# 转换为PyTorch张量\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "X_cat_train_tensor = torch.tensor(X_cat_train, dtype=torch.float32)\n",
    "X_cont_train_tensor = torch.tensor(X_cont_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_cat_test_tensor = torch.tensor(X_cat_test, dtype=torch.float32)\n",
    "X_cont_test_tensor = torch.tensor(X_cont_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# 创建DataLoader\n",
    "train_dataset = TimeSeriesDataset(X_cat_train_tensor, X_cont_train_tensor, y_train_tensor)\n",
    "test_dataset = TimeSeriesDataset(X_cat_test_tensor, X_cont_test_tensor, y_test_tensor)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False)  # 时间序列数据通常不应该打乱\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the nunique of each categorical variable\n",
    "num_unique_site_codes = data['SiteCode'].nunique()\n",
    "num_unique_local_authority_codes = data['LocalAuthorityCode'].nunique()\n",
    "num_unique_site_types = data['SiteType'].nunique()\n",
    "num_unique_species_types = data['SpeciesType'].nunique()\n",
    "num_unique_sat_bands = data['SatBand'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyLSTMModel(nn.Module):\n",
    "    def __init__(self, num_continuous, embedding_dim_dict, lstm_hidden_dim, lstm_layers, output_size):\n",
    "        super(MyLSTMModel, self).__init__()\n",
    "        \n",
    "        # Embedding Layer\n",
    "        self.site_code_embedding = nn.Embedding(num_embeddings=embedding_dim_dict['SiteCode'][0], embedding_dim=embedding_dim_dict['SiteCode'][1])\n",
    "        self.local_authority_code_embedding = nn.Embedding(num_embeddings=embedding_dim_dict['LocalAuthorityCode'][0], embedding_dim=embedding_dim_dict['LocalAuthorityCode'][1])\n",
    "        self.site_type_embedding = nn.Embedding(num_embeddings=embedding_dim_dict['SiteType'][0], embedding_dim=embedding_dim_dict['SiteType'][1])\n",
    "        self.species_type_embedding = nn.Embedding(num_embeddings=embedding_dim_dict['SpeciesType'][0], embedding_dim=embedding_dim_dict['SpeciesType'][1])\n",
    "        self.sat_band_embedding = nn.Embedding(num_embeddings=embedding_dim_dict['SatBand'][0], embedding_dim=embedding_dim_dict['SatBand'][1])\n",
    "        \n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_size=sum([ed[1] for ed in embedding_dim_dict.values()]) + num_continuous,\n",
    "                            hidden_size=lstm_hidden_dim,\n",
    "                            num_layers=lstm_layers,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        self.linear = nn.Linear(lstm_hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x_categorical, x_continuous):\n",
    "        \n",
    "        # 打印x_categorical的最小和最大值\n",
    "        print(\"x_categorical min:\", x_categorical.min(dim=0))\n",
    "        print(\"x_categorical max:\", x_categorical.max(dim=0))\n",
    "\n",
    "        # 将分类特征的浮点张量转换为长整型张量\n",
    "        \n",
    "        \n",
    "        x_categorical = x_categorical.long()  # 这是关键的转换步骤\n",
    "        \n",
    "        embeddings = [\n",
    "            self.site_code_embedding(x_categorical[:, 0]),\n",
    "            self.local_authority_code_embedding(x_categorical[:, 1]),\n",
    "            self.site_type_embedding(x_categorical[:, 2]),\n",
    "            self.species_type_embedding(x_categorical[:, 3]),\n",
    "            self.sat_band_embedding(x_categorical[:, 4])\n",
    "        ]\n",
    "\n",
    "        # Merging along with the feature's dimension\n",
    "        x_embedded = torch.cat(embeddings, dim=2)  \n",
    "        # merging with continuous features\n",
    "        x_combined = torch.cat([x_embedded, x_continuous], dim=2)  \n",
    "        \n",
    "        lstm_out, (h_n, c_n) = self.lstm(x_combined)\n",
    "        output = self.linear(lstm_out[:, -1, :])  \n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "number_of_continuous_features = 12\n",
    "embedding_dim_dict={\n",
    "    'SiteCode': (num_unique_site_codes, 98),\n",
    "    'LocalAuthorityCode': (num_unique_local_authority_codes, 30),\n",
    "    'SiteType': (num_unique_site_types, 6),\n",
    "    'SpeciesType': (num_unique_species_types, 5),\n",
    "    'SatBand': (num_unique_sat_bands, 6)\n",
    "}\n",
    "lstm_hidden_dim = 128\n",
    "lstm_layers = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实例化模型 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_categorical shape: torch.Size([32, 24, 5])\n",
      "x_continuous shape: torch.Size([32, 24, 12])\n",
      "y shape: torch.Size([32, 1])\n",
      "x_categorical dtype: torch.float32\n",
      "x_continuous dtype: torch.float32\n",
      "y dtype: torch.float32\n",
      "x_categorical min: torch.return_types.min(\n",
      "values=tensor([[83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.]]),\n",
      "indices=tensor([[0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0]]))\n",
      "x_categorical max: torch.return_types.max(\n",
      "values=tensor([[83., 25.,  0.,  0.,  1.],\n",
      "        [83., 25.,  0.,  0.,  1.],\n",
      "        [83., 25.,  0.,  0.,  1.],\n",
      "        [83., 25.,  0.,  0.,  1.],\n",
      "        [83., 25.,  0.,  0.,  1.],\n",
      "        [83., 25.,  0.,  0.,  1.],\n",
      "        [83., 25.,  0.,  0.,  1.],\n",
      "        [83., 25.,  0.,  0.,  1.],\n",
      "        [83., 25.,  0.,  0.,  2.],\n",
      "        [83., 25.,  0.,  0.,  2.],\n",
      "        [83., 25.,  0.,  0.,  2.],\n",
      "        [83., 25.,  0.,  0.,  2.],\n",
      "        [83., 25.,  0.,  0.,  2.],\n",
      "        [83., 25.,  0.,  0.,  2.],\n",
      "        [83., 25.,  0.,  0.,  2.],\n",
      "        [83., 25.,  0.,  0.,  2.],\n",
      "        [83., 25.,  0.,  0.,  2.],\n",
      "        [83., 25.,  0.,  0.,  2.],\n",
      "        [83., 25.,  0.,  0.,  2.],\n",
      "        [83., 25.,  0.,  0.,  2.],\n",
      "        [83., 25.,  0.,  0.,  2.],\n",
      "        [83., 25.,  0.,  0.,  2.],\n",
      "        [83., 25.,  0.,  0.,  2.],\n",
      "        [83., 25.,  0.,  0.,  2.]]),\n",
      "indices=tensor([[ 0,  0,  0,  0, 18],\n",
      "        [ 0,  0,  0,  0, 17],\n",
      "        [ 0,  0,  0,  0, 16],\n",
      "        [ 0,  0,  0,  0, 15],\n",
      "        [ 0,  0,  0,  0, 14],\n",
      "        [ 0,  0,  0,  0, 13],\n",
      "        [ 0,  0,  0,  0, 12],\n",
      "        [ 0,  0,  0,  0, 11],\n",
      "        [ 0,  0,  0,  0, 31],\n",
      "        [ 0,  0,  0,  0, 30],\n",
      "        [ 0,  0,  0,  0, 29],\n",
      "        [ 0,  0,  0,  0, 28],\n",
      "        [ 0,  0,  0,  0, 27],\n",
      "        [ 0,  0,  0,  0, 26],\n",
      "        [ 0,  0,  0,  0, 25],\n",
      "        [ 0,  0,  0,  0, 24],\n",
      "        [ 0,  0,  0,  0, 23],\n",
      "        [ 0,  0,  0,  0, 22],\n",
      "        [ 0,  0,  0,  0, 21],\n",
      "        [ 0,  0,  0,  0, 20],\n",
      "        [ 0,  0,  0,  0, 19],\n",
      "        [ 0,  0,  0,  0, 18],\n",
      "        [ 0,  0,  0,  0, 17],\n",
      "        [ 0,  0,  0,  0, 16]]))\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[124], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my dtype: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 23\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_categorical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_continuous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, y)\n\u001b[0;32m     25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\SBH\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SBH\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[117], line 36\u001b[0m, in \u001b[0;36mMyLSTMModel.forward\u001b[1;34m(self, x_categorical, x_continuous)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# 将分类特征的浮点张量转换为长整型张量\u001b[39;00m\n\u001b[0;32m     33\u001b[0m x_categorical \u001b[38;5;241m=\u001b[39m x_categorical\u001b[38;5;241m.\u001b[39mlong()  \u001b[38;5;66;03m# 这是关键的转换步骤\u001b[39;00m\n\u001b[0;32m     34\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msite_code_embedding(x_categorical[:, \u001b[38;5;241m0\u001b[39m]),\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_authority_code_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_categorical\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msite_type_embedding(x_categorical[:, \u001b[38;5;241m2\u001b[39m]),\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecies_type_embedding(x_categorical[:, \u001b[38;5;241m3\u001b[39m]),\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msat_band_embedding(x_categorical[:, \u001b[38;5;241m4\u001b[39m])\n\u001b[0;32m     40\u001b[0m ]\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Merging along with the feature's dimension\u001b[39;00m\n\u001b[0;32m     43\u001b[0m x_embedded \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(embeddings, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \n",
      "File \u001b[1;32mc:\\Users\\SBH\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SBH\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\SBH\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SBH\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2227\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2228\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2229\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2230\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2231\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "model = MyLSTMModel(num_continuous=number_of_continuous_features,\n",
    "                    embedding_dim_dict=embedding_dim_dict,\n",
    "                    lstm_hidden_dim=lstm_hidden_dim,\n",
    "                    lstm_layers=lstm_layers,\n",
    "                    output_size=1)\n",
    "\n",
    "criterion = nn.MSELoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for i, (x_categorical, x_continuous, y) in enumerate(train_dataloader):\n",
    "        if i == 0:  # 仅在每个epoch的开始打印一次\n",
    "            print(f\"x_categorical shape: {x_categorical.shape}\")\n",
    "            print(f\"x_continuous shape: {x_continuous.shape}\")\n",
    "            print(f\"y shape: {y.shape}\")\n",
    "            # 打印数据类型也很重要\n",
    "            print(f\"x_categorical dtype: {x_categorical.dtype}\")\n",
    "            print(f\"x_continuous dtype: {x_continuous.dtype}\")\n",
    "            print(f\"y dtype: {y.dtype}\")\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x_categorical, x_continuous)\n",
    "        loss = criterion(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_categorical min: torch.return_types.min(\n",
      "values=tensor([[83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.],\n",
      "        [83., 25.,  0.,  0.,  0.]]),\n",
      "indices=tensor([[0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0]]))\n",
      "x_categorical max: torch.return_types.max(\n",
      "values=tensor([[83., 25.,  0.,  0.,  1.],\n",
      "        [83., 25.,  0.,  0.,  1.],\n",
      "        [83., 25.,  0.,  0.,  1.],\n",
      "        [83., 25.,  0.,  0.,  1.],\n",
      "        [83., 25.,  0.,  0.,  1.],\n",
      "        [83., 25.,  0.,  0.,  1.],\n",
      "        [83., 25.,  0.,  0.,  1.],\n",
      "        [83., 25.,  0.,  0.,  1.],\n",
      "        [83., 25.,  0.,  0.,  2.],\n",
      "        [83., 25.,  0.,  0.,  2.],\n",
      "        [83., 25.,  0.,  0.,  2.],\n",
      "        [83., 25.,  0.,  0.,  2.],\n",
      "        [83., 25.,  0.,  0.,  2.],\n",
      "        [83., 25.,  0.,  0.,  2.],\n",
      "        [83., 25.,  0.,  0.,  2.],\n",
      "        [83., 25.,  0.,  0.,  2.],\n",
      "        [83., 25.,  0.,  0.,  2.],\n",
      "        [83., 25.,  0.,  0.,  2.],\n",
      "        [83., 25.,  0.,  0.,  2.],\n",
      "        [83., 25.,  0.,  0.,  2.],\n",
      "        [83., 25.,  0.,  0.,  2.],\n",
      "        [83., 25.,  0.,  0.,  2.],\n",
      "        [83., 25.,  0.,  0.,  2.],\n",
      "        [83., 25.,  0.,  0.,  2.]]),\n",
      "indices=tensor([[ 0,  0,  0,  0, 18],\n",
      "        [ 0,  0,  0,  0, 17],\n",
      "        [ 0,  0,  0,  0, 16],\n",
      "        [ 0,  0,  0,  0, 15],\n",
      "        [ 0,  0,  0,  0, 14],\n",
      "        [ 0,  0,  0,  0, 13],\n",
      "        [ 0,  0,  0,  0, 12],\n",
      "        [ 0,  0,  0,  0, 11],\n",
      "        [ 0,  0,  0,  0, 31],\n",
      "        [ 0,  0,  0,  0, 30],\n",
      "        [ 0,  0,  0,  0, 29],\n",
      "        [ 0,  0,  0,  0, 28],\n",
      "        [ 0,  0,  0,  0, 27],\n",
      "        [ 0,  0,  0,  0, 26],\n",
      "        [ 0,  0,  0,  0, 25],\n",
      "        [ 0,  0,  0,  0, 24],\n",
      "        [ 0,  0,  0,  0, 23],\n",
      "        [ 0,  0,  0,  0, 22],\n",
      "        [ 0,  0,  0,  0, 21],\n",
      "        [ 0,  0,  0,  0, 20],\n",
      "        [ 0,  0,  0,  0, 19],\n",
      "        [ 0,  0,  0,  0, 18],\n",
      "        [ 0,  0,  0,  0, 17],\n",
      "        [ 0,  0,  0,  0, 16]]))\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[123], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_categorical, x_continuous, y \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[0;32m     14\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 15\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_categorical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_continuous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(predictions, y)\n\u001b[0;32m     17\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\SBH\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SBH\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[117], line 36\u001b[0m, in \u001b[0;36mMyLSTMModel.forward\u001b[1;34m(self, x_categorical, x_continuous)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# 将分类特征的浮点张量转换为长整型张量\u001b[39;00m\n\u001b[0;32m     33\u001b[0m x_categorical \u001b[38;5;241m=\u001b[39m x_categorical\u001b[38;5;241m.\u001b[39mlong()  \u001b[38;5;66;03m# 这是关键的转换步骤\u001b[39;00m\n\u001b[0;32m     34\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msite_code_embedding(x_categorical[:, \u001b[38;5;241m0\u001b[39m]),\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_authority_code_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_categorical\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msite_type_embedding(x_categorical[:, \u001b[38;5;241m2\u001b[39m]),\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecies_type_embedding(x_categorical[:, \u001b[38;5;241m3\u001b[39m]),\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msat_band_embedding(x_categorical[:, \u001b[38;5;241m4\u001b[39m])\n\u001b[0;32m     40\u001b[0m ]\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Merging along with the feature's dimension\u001b[39;00m\n\u001b[0;32m     43\u001b[0m x_embedded \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(embeddings, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \n",
      "File \u001b[1;32mc:\\Users\\SBH\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SBH\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\SBH\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SBH\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2227\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2228\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2229\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2230\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2231\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "model = MyLSTMModel(num_continuous=number_of_continuous_features,\n",
    "                    embedding_dim_dict=embedding_dim_dict,\n",
    "                    lstm_hidden_dim=lstm_hidden_dim,\n",
    "                    lstm_layers=lstm_layers,\n",
    "                    output_size=1)\n",
    "\n",
    "criterion = nn.MSELoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for x_categorical, x_continuous, y in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x_categorical, x_continuous)\n",
    "        loss = criterion(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # 假设有test_dataloader\n",
    "    total_loss = 0\n",
    "    for x_categorical, x_continuous, y in test_dataloader:\n",
    "        predictions = model(x_categorical, x_continuous)\n",
    "        loss = criterion(predictions, y)\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Test Loss: {total_loss / len(test_dataloader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyLSTMModel(input_dim=X_train.shape[1], hidden_dim=500)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "model.to(device)\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        # 将数据移到GPU\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "actuals = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs.unsqueeze(1))\n",
    "        predictions.append(outputs.numpy())\n",
    "        actuals.append(labels.numpy())\n",
    "\n",
    "predictions = np.vstack(predictions)\n",
    "actuals = np.vstack(actuals)\n",
    "\n",
    "# 计算MSE\n",
    "mse = mean_squared_error(actuals, predictions)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# 绘制残差图\n",
    "residuals = actuals - predictions\n",
    "plt.scatter(actuals, residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='-')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
